
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="bootstrap.js"></script>
<script type="text/javascript" charset="utf-8" src="https://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script> 
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
}

@media screen and (min-width: 980px){
    body {
        width: 980px;
    }
}

h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #5364cc;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1 {
    text-align: center;
}
h2,h3 {
    text-align: left;
}

h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
h3 {
    font-weight: 600;
    margin: 16px 0px 4px 0px;
}

.paper-title {
    padding: 1px 0px 1px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.col-1 {
     width: 100%;
     float: left;
}

.author-row, .affil-row {
    font-size: 26px;
}

.author-row-new { 
    text-align: center; 
}

.author-row-new a {
    display: inline-block;
    font-size: 20px;
    padding: 4px;
}

.author-row-new sup {
    color: #313436;
    font-size: 12px;
}

.affiliations-new {
    font-size: 18px;
    text-align: center;
    width: 80%;
    margin: 0 auto;
    margin-bottom: 20px;
}

.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 26px;
}
.affiliatons {
    font-size: 18px;
}
.affil-row {
    margin-top: 18px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    color: #666;
    margin-top: 4px;
    margin-bottom: 10px;
}


video {
    display: block;
    margin: auto;
}


figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    padding: 0;
    margin: 0;
    list-style: none;
}

.paper-btn-coming-soon {
    position: relative; 
    top: 0;
    left: 0;
}

.coming-soon {
    position: absolute;
    top: -15px;
    right: -15px;
}

.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #5364cc;
  color: white !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}
.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}

.paper-btn:hover {
    opacity: 0.85;
}

.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}

.venue {
    font-size: 23px;
}

.topnav {
    background-color: #EEEEEE;
    overflow: hidden;
}

.topnav div {
    max-width: 1070px;
    margin: 0 auto;
}

.topnav a {
    display: inline-block;
    color: black;
    text-align: center;
    vertical-align: middle;
    padding: 16px 16px;
    text-decoration: none;
    font-size: 18px;
}

.topnav img {
    padding: 2px 0px;
    width: 100%;
    margin: 0.2em 0px 0.3em 0px;
    vertical-align: middle;
}

pre {
    font-size: 0.9em;
    padding-left: 7px;
    padding-right: 7px;
    padding-top: 3px;
    padding-bottom: 3px;
    border-radius: 3px;
    background-color: rgb(235, 235, 235);
    overflow-x: auto;
}

.download-thumb {
    display: flex;
}

@media only screen and (max-width: 620px) {
    .download-thumb {
        display: none;
    }
}

.paper-stuff {
    width: 50%;
    font-size: 20px;
}

@media only screen and (max-width: 620px) {
    .paper-stuff {
        width: 100%;
    }
}
* {
  box-sizing: border-box;
}

.column {
  text-align: center;
  float: left;
  width: 16.666%;
  padding: 5px;
}
.column3 {
  text-align: center;
  float: left;
  width: 33.333%;
  padding: 5px;
}
.column4 {
  text-align: center;
  float: left;
  width: 50%;
  padding: 5px;
}
.column5 {
  text-align: center;
  float: left;
  width: 20%;
  padding: 5px;
}
.border-right {
    border-right: 1px solid black;
}
.border-bottom{
    border-bottom: 1px solid black;
}



/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}
.img-fluid {
  max-width: 100%;
  height: auto;
}
.figure-img {
  margin-bottom: 0.5rem;
  line-height: 1;
}








.rounded-circle {
  border-radius: 50% !important;
}






/* Responsive layout - makes the three columns stack on top of each other instead of next to each other */
@media screen and (max-width: 500px) {
  .column {
    width: 100%;
  }
}
@media screen and (max-width: 500px) {
  .column3 {
    width: 100%;
  }
}

</style>
<link rel="stylesheet" href="bootstrap-grid.css">

<script type="text/javascript" src="../js/hidebib.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
    <head>
        <title>Monocular 3D Object Detection with Bounding Box Denoising in 3D by Perceiver</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:description" content="Monocular 3D Object Detection with Bounding Box Denoising in 3D by Perceiver"/>
        <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
        <meta name="twitter:title" content="Monocular 3D Object Detection with Bounding Box Denoising in 3D by Perceiver">
        <meta name="twitter:description" content="">
        <meta name="twitter:image" content="">
    </head>

 <body>


<div class="container">
    <div class="paper-title">
    <h1> 
        Monocular 3D Object Detection with Bounding Box Denoising in 3D by Perceiver
    </div>

    <div id="authors">
        <center>
            <div class="author-row-new">
                <a href="https://xianpeng919.github.io/">Xianpeng Liu<sup>1</sup></a>,
                <a href="https://zczcwh.github.io/">Ce Zheng<sup>2</sup></a>,
                <a href="https://scholar.google.com/citations?user=6_h1y4wAAAAJ&hl=en/">Kelvin Cheng<sup>1</sup></a>,
                <a href="https://xuenan.net//">Nan Xue<sup>5</sup></a>,
                <a href="http://maple-lab.net/gqi/">Guo-Jun Qi<sup>3,4</sup></a>,
                <a href="https://tfwu.github.io/">Tianfu Wu<sup>1</sup></a>,
            </div>
        </center>
        <center>
        <div class="affiliations">
            <span><sup>1</sup> North Carolina State University </span><br/>
            <span><sup>2</sup> Center for Research in Computer Vision, University of Central Florida </span><br/>
            <span><sup>3</sup> OPPO Seattle Research Center, USA </span>
            <span><sup>4</sup> Westlake University </span><br/>
            <span><sup>5</sup> Ant Research </span><br/>
        </div>

<!--         <br>*indicates equal contribution. -->

        <div class="affil-row">
            <div class="venue text-center"><b> ICCV 2023 </b></div>
        </div>

        </center>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="paper-btn" href="https://arxiv.org/abs/2304.01289">
                <span class="material-icons"> description </span> 
                 Paper
            </a>
            <a class="paper-btn" href="">
                <span class="material-icons"> code_TBD </span>
                Code
            </a>
<!--             <a class="paper-btn" href="https://www.youtube.com/watch?v=-O4V-yqJmms">
                <span class="material-icons"> code </span>
                Video
            </a>   -->
        </div>
    </div>

    
    <!-- <section id="teaser-image">
        <center>
            <figure>
                <video class="centered" width="80%" autoplay loop muted playsinline class="video-background " >
                    <source src="assets/LION_video_v10.mp4#t=0.001" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </figure>

        </center>
    </section>
     -->
   
        
    </section>
    <section id="teaser-image">
        <hr>
            <div class="mx-auto">
                <center><img class="card-img-top" src="materials/monoxiver.gif" style="width:350px"></center>
            </div>
        <hr>
    
    

    
    <section id="abstract"/>
        <hr>
        <h2>Abstract</h2>
        <div class="flex-row">
            <p>
                The main challenge of monocular 3D object detection is the accurate localization of 3D center. 
                Motivated by a new and strong observation that this challenge can be remedied by a 3D-space local-grid search scheme in an ideal case, 
                we propose a stage-wise approach, which combines the information flow from 2D-to-3D (3D bounding box proposal generation with a single 2D image) and 3D-to-2D (proposal verification by denoising with 3D-to-2D contexts) in a top-down manner.
                Specifically, we first obtain initial proposals from off-the-shelf backbone monocular 3D detectors. 
                Then, we generate a 3D anchor space by local-grid sampling from the initial proposals. 
                Finally, we perform 3D bounding box denoising at the 3D-to-2D proposal verification stage.
                To effectively learn discriminative features for denoising highly overlapped proposals, 
                this paper presents a method of using the Perceiver I/O model to fuse the 3D-to-2D geometric information and the 2D appearance information.
                With the encoded latent representation of a proposal, the verification head is implemented with a self-attention module. 
                Our method, named as MonoXiver, is generic and can be easily adapted to any backbone monocular 3D detectors.
                Experimental results on the well-established KITTI dataset and the challenging large-scale Waymo dataset show that MonoXiver consistently achieves improvement with limited computation overhead.             </p>
        </div>
    </section>
    <section id="method"/>
        <hr>
        <h2>MonoXiver</h2>

            <br><br>

            <div class="mx-auto">
<!--                <left><p>The proposed framework that composes a "generator" and an ensemble of "scorers" through iterative consensus enables zero-shot generalization across a variety of multimodal tasks.</p></left>-->
                <center><img class="card-img-top" src="materials/framework_compare.jpg" style="width:950px"></center>
            </div>

            <br><br><br>

            <!-- <div class="row">
                    <div class="column4">
                        <center><img class="card-img-top" src="materials/framework.png" style="width:400px"></center>
                    </div>

                    <div class="column4">
                        <video width="400" loop autoplay muted>
                            <source src="materials/teaser3.mp4" type="video/mp4">
                        </video>
                    </div>
            </div>
 -->
<!--            <div class="flex-row">-->
<!--                <div class="mx-auto">-->
<!--                    <left><p><b>Overview of the proposed unified framework.</b> Dashed lines are omitted for certain tasks. Orange lines represent the components used to refine the generated result.</p></left>-->
<!--                    <br>-->
<!--                    <center><img class="card-img-top" src="materials/framework.png" style="width:400px"></center>-->

<!--                    <br><br>-->

<!--                    &lt;!&ndash; <video width="800" loop autoplay muted style="border:1px solid black"> &ndash;&gt;-->
<!--                    <video width="850" loop autoplay muted controls>-->
<!--                        <source src="materials/new3-2.mp4" type="video/mp4">-->
<!--                    </video>-->
<!--                </div>-->

<!--                <br><br>-->
<!--                <p><b>Image generation: </b> A pre-trained diffusion model is used as the generator, and multiple scorers, such as CLIP and image classifiers, are used to provide feedback to the generator.</p>-->
<!--                <p><b>Video question answering: </b> GPT-2 is used as the generator, and a set of CLIP models are used as scorers.</p>-->
<!--                <p><b>Grade school math: </b> GPT-2 is used as the generator, and a set of question-solution classifiers are used as scorers.</p>-->
<!--                <p><b>Robot manipulation: </b> MPC+World model is used as the generator, and a pre-trained image segmentation model is used to compute the scores from multiple camera views to select the best action.</p>-->

<!--            </div>-->
    </section>
        

    <section id="results">
        <hr>
        <h2>Results of image classification task </h2>
            <br><br>
            <div class="mx-auto">
                <center><img class="card-img-top" src="materials/table_img.jpg" style="width:950px"></center>
            </div>
        <hr>

        <h2>Results of human mesh recovery</h2>
            <br><br>
            <div class="mx-auto">
                <center><img class="card-img-top" src="materials/table_hmr.jpg" style="width:950px"></center>
                 <br><br>
                <center><img class="card-img-top" src="materials/MG_PA.jpg" style="width:550px"></center>
            </div>
        <hr>

        <h2>Mesh visualization</h2>
            <br><br>
            <div class="mx-auto">
                <center><p><b>Frame-by-frame reconstruction for the video input</b></p></center>
                <center><img class="card-img-top" src="materials/supp_vis_video.jpg" style="width:100%"></center>
            </div>
            <br><br>
            <div class="mx-auto">
                <center><p><b>Qualitative comparison with SOTA method METRO</b></p></center>
                <center><img class="card-img-top" src="materials/supp_vis_comp.jpg" style="width:100%"></center>
            </div>
            <br><br>
            <div class="mx-auto">
                <center><p><b>Hand mesh visualization</b></p></center>
                <center><img class="card-img-top" src="materials/supp_vis_hand.jpg" style="width:100%"></center>
            </div>
            <br><br><br>
        <hr>

    </section>

<!--     <section id="paper">
        <h2> Video </h2>
    <div class="video-container">
        <iframe width="896" height="504" src="https://www.youtube.com/embed/-O4V-yqJmms" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
    </div></br>     -->
   
    <section id="paper">
        <h2>Bibtex</h2>
        <div class="page-body"><pre id="ad6975be-3353-467d-ae48-6313d767ffa6" class="code"><code>
            @InProceedings{liu2023monoxiver,
                title={Monocular 3D Object Detection with Bounding Box Denoising in 3D by Perceiver},
                author={Liu, Xianpeng and Zheng, Ce and Kelvin Cheng and Nan Xue and Qi, Guo-Jun and Tianfu Wu},
                booktitle ={2023 IEEE/CVF International Conference on Computer Vision (ICCV)},
                year={2023}
            }
        </code></pre><p id="1a3aa306-c4b8-4872-8fb0-411495c73d55" class="">
        </p></div>

    </section>


    <section>
        This webpage template was adapted from <a href='https://nv-tlabs.github.io/LION/'>here</a>.
    </section>
    


</div>
</body>
</html>
